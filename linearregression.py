# -*- coding: utf-8 -*-
"""linearRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Np_jzfST_hSargy_F_okVRAkRlGMIjG

## Import the Spark SQL and Spark ML Libraries

Import the libaries you will need
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.ml import Pipeline
from pyspark.ml.regression import GBTRegressor, LinearRegression, RandomForestRegressor
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vectors
from pyspark.sql.functions import col, udf, when
from pyspark.sql.types import IntegerType, DoubleType
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator

PYSPARK_CLI = True
if PYSPARK_CLI:
    sc = SparkContext.getOrCreate()
    spark = SparkSession(sc)

# Function to convert ISO 8601 durations to minutes
def parse_duration(duration):
    import re
    if duration is None:
        return None  # Or handle null values appropriately
    if not isinstance(duration, str):
        return None  # Or handle non-string values appropriately
    match = re.match(r'PT(\d+H)?(\d+M)?', duration)
    if match:
        hours = int(match.group(1)[:-1]) if match.group(1) else 0
        minutes = int(match.group(2)[:-1]) if match.group(2) else 0
        return hours * 60 + minutes
    else:
        return None  # Or handle unmatched patterns
# Register UDF
parse_duration_udf = udf(parse_duration, IntegerType())

# File location and type
file_location = "/user/skarri2/flights_LAX.csv"
file_type = "csv"

# CSV options
infer_schema = "True"
first_row_is_header = "True"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

print(df)

# Apply the UDF and handle any potential nulls or type issues
df = df.withColumn("flightDayOfYear", dayofyear(col("flightDate").cast("date")))
df = df.withColumn("travelDurationMin", parse_duration_udf(col("travelDuration")).cast(DoubleType()))
df = df.withColumn("flightMonth", month("flightDate"))
df = df.withColumn("flightYear", year("flightDate"))
df = df.withColumn("SearchDayoftheYear", dayofyear(col("searchDate").cast("date")))
df = df.na.fill(0)  # Fill nulls if any

# Drop the columns that are not going to be used
df = df.drop("legID","segmentsDepartureTimeEpochSeconds","segmentsArrivalTimeEpochSeconds","segmentsArrivalAirportCode","segmentsDepartureAirportCode","segmentsAirlineName","segmentsAirlineCode","segmentsEquipmentDescription","segmentsDurationInSeconds","segmentsDistance","segmentsCabinCode","segmentsCabinCode","segmentsDistance","segmentsDurationInSeconds","segmentsArrivalTimeRaw","segmentsDepartureTimeRaw","segmentsAirlineCode:","baseFare","startingAirport",)

df.printSchema()

# StringIndexer for airport codes
indexer =StringIndexer(inputCols=["destinationAirport","fareBasisCode"], outputCols=["destinationAirportIdx3","fareBasisCodeIndexed"])

df = indexer.fit(df).transform(df)

# print the schema of the DataFrame, df, showing the data types of each column
df.printSchema()
#  display the first 10 rows of the DataFrame in a tabular format

# Define the assembler
assembler = VectorAssembler(
    inputCols=[
        "flightDayOfYear",
        "isRefundable",
        "isNonStop",
        "elapsedDays",
        "isBasicEconomy",
        "destinationAirportIdx3",
        "seatsRemaining",
        "totalTravelDistance",
        "travelDurationMin",
        "fareBasisCodeIndexed",
        "SearchDayoftheYear"
    ],
    outputCol="features"
)

# Apply the assembler to the training data
#train_features = assembler.transform(train)

splits = df.randomSplit([0.7, 0.3])
train = splits[0]
test = splits[1]
train_rows = train.count()
test_rows = test.count()
print("Training Rows:", train_rows, " Testing Rows:", test_rows)

# Apply the assembler to the training data
train_features = assembler.transform(train)

# Create a Random Forest Regression model
# numTrees: sets the number of decision trees to be grown in the Random Forest
# seed: sets the seed for the random number generator used by the model
lr = LinearRegression(featuresCol="features", labelCol="totalFare")

paramGrid = (ParamGridBuilder()
             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
             .addGrid(lr.regParam, [0.01, 0.1, 0.5])
             .addGrid(lr.fitIntercept, [True, False])  # Whether to fit the intercept term
             .addGrid(lr.maxIter, [10, 50, 100])  # Maximum number of iterations
             .build())  #0.13

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction",
 labelCol="totalFare",metricName="r2")

pipeline = Pipeline(stages=[assembler, lr])

# Define the TrainValidationSplit
tv = TrainValidationSplit(estimator=lr, estimatorParamMaps=[{}], evaluator=RegressionEvaluator(labelCol="totalFare"))

# Fit the model
model = tv.fit(train_features)
# Fit the model using CrossValidator
# cvModel = crossval.fit(train)  # Assuming 'train' is your training dataset

# Apply the assembler to the test data
test_features = assembler.transform(test)

# Make predictions
prediction = model.transform(test_features)

# Select relevant columns
predicted = prediction.select("features", "prediction", "totalFare")

# Show the predicted DataFrame
predicted.show()

# Make predictions and evaluate the model
# prediction = cvModel.transform(test)  # Assuming 'test' is your test dataset
# predicted = prediction.select("features", "prediction", "totalFare")
# predicted.show()

display(predicted)

# print("R Squared (R2) on test data = %g" %lr_evaluator.evaluate(prediction))
# lr_evaluator = RegressionEvaluator(labelCol="totalFare",
# predictionCol="prediction", metricName="rmse")
# print("RMSE: %f" % lr_evaluator.evaluate(prediction))
lr_evaluator = RegressionEvaluator(labelCol="totalFare",
predictionCol="prediction", metricName="r2")
print("R Squared (R2) on test data = %g" %lr_evaluator.evaluate(prediction))
lr_evaluator.setMetricName("rmse")
rmse = lr_evaluator.evaluate(prediction)
print("RMSE on test data = %f" % rmse)